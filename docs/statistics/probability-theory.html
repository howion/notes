<!DOCTYPE html><html xmlns="http://www.w3.org/1999/xhtml" lang="" xml:lang=""><meta charset="utf-8"><link href="https://www.howion.com/favicon.svg" rel="icon" type="image/svg+xml"><meta name="generator" content="pandoc"><meta name="viewport" content="width=device-width,initial-scale=1,user-scalable=yes"><title>Probability Theory</title><style>html{color:#1a1a1a;background-color:#fdfdfd}body{margin:0 auto;max-width:36em;padding-left:50px;padding-right:50px;padding-top:50px;padding-bottom:50px;hyphens:auto;overflow-wrap:break-word;text-rendering:optimizeLegibility;font-kerning:normal}@media (max-width:600px){body{font-size:.9em;padding:12px}h1{font-size:1.8em}}@media print{html{background-color:#fff}body{background-color:transparent;color:#000;font-size:12pt}h2,h3,p{orphans:3;widows:3}h2,h3,h4{page-break-after:avoid}}p{margin:1em 0}a{color:#1a1a1a}a:visited{color:#1a1a1a}img{max-width:100%}svg{height:auto;max-width:100%}h1,h2,h3,h4,h5,h6{margin-top:1.4em}h5,h6{font-size:1em;font-style:italic}h6{font-weight:400}ol,ul{padding-left:1.7em;margin-top:1em}li>ol,li>ul{margin-top:0}blockquote{margin:1em 0 1em 1.7em;padding-left:1em;border-left:2px solid #e6e6e6;color:#606060}code{font-family:Menlo,Monaco,Consolas,'Lucida Console',monospace;font-size:85%;margin:0;hyphens:manual}pre{margin:1em 0;overflow:auto}pre code{padding:0;overflow:visible;overflow-wrap:normal}.sourceCode{background-color:transparent;overflow:visible}hr{border:none;border-top:1px solid #1a1a1a;height:1px;margin:1em 0}table{margin:1em 0;border-collapse:collapse;width:100%;overflow-x:auto;display:block;font-variant-numeric:lining-nums tabular-nums}table caption{margin-bottom:.75em}tbody{margin-top:.5em;border-top:1px solid #1a1a1a;border-bottom:1px solid #1a1a1a}th{border-top:1px solid #1a1a1a;padding:.25em .5em .25em .5em}td{padding:.125em .5em .25em .5em}header{margin-bottom:4em;text-align:center}#TOC li{list-style:none}#TOC ul{padding-left:1.3em}#TOC>ul{padding-left:0}#TOC a:not(:hover){text-decoration:none}code{white-space:pre-wrap}span.smallcaps{font-variant:small-caps}div.columns{display:flex;gap:min(4vw,1.5em)}div.column{flex:auto;overflow-x:auto}div.hanging-indent{margin-left:1.5em;text-indent:-1.5em}ul.task-list[class]{list-style:none}ul.task-list li input[type=checkbox]{font-size:inherit;width:.8em;margin:0 .8em .2em -1.6em;vertical-align:middle}html{-webkit-text-size-adjust:100%}pre>code.sourceCode{white-space:pre;position:relative}pre>code.sourceCode>span{display:inline-block;line-height:1.25}pre>code.sourceCode>span:empty{height:1.2em}.sourceCode{overflow:visible}code.sourceCode>span{color:inherit;text-decoration:inherit}div.sourceCode{margin:1em 0}pre.sourceCode{margin:0}@media screen{div.sourceCode{overflow:auto}}@media print{pre>code.sourceCode{white-space:pre-wrap}pre>code.sourceCode>span{text-indent:-5em;padding-left:5em}}pre.numberSource code{counter-reset:source-line 0}pre.numberSource code>span{position:relative;left:-4em;counter-increment:source-line}pre.numberSource code>span>a:first-child::before{content:counter(source-line);position:relative;left:-1em;text-align:right;vertical-align:baseline;border:none;display:inline-block;-webkit-touch-callout:none;-webkit-user-select:none;-khtml-user-select:none;-moz-user-select:none;-ms-user-select:none;user-select:none;padding:0 4px;width:4em;color:#aaa}pre.numberSource{margin-left:3em;border-left:1px solid #aaa;padding-left:4px}@media screen{pre>code.sourceCode>span>a:first-child::before{text-decoration:underline}}code span.al{color:red;font-weight:700}code span.an{color:#60a0b0;font-weight:700;font-style:italic}code span.at{color:#7d9029}code span.bn{color:#40a070}code span.bu{color:green}code span.cf{color:#007020;font-weight:700}code span.ch{color:#4070a0}code span.cn{color:#800}code span.co{color:#60a0b0;font-style:italic}code span.cv{color:#60a0b0;font-weight:700;font-style:italic}code span.do{color:#ba2121;font-style:italic}code span.dt{color:#902000}code span.dv{color:#40a070}code span.er{color:red;font-weight:700}code span.fl{color:#40a070}code span.fu{color:#06287e}code span.im{color:green;font-weight:700}code span.in{color:#60a0b0;font-weight:700;font-style:italic}code span.kw{color:#007020;font-weight:700}code span.op{color:#666}code span.ot{color:#007020}code span.pp{color:#bc7a00}code span.sc{color:#4070a0}code span.ss{color:#b68}code span.st{color:#4070a0}code span.va{color:#19177c}code span.vs{color:#4070a0}code span.wa{color:#60a0b0;font-weight:700;font-style:italic}</style><style>@import url(https://fonts.googleapis.com/css2?family=Inter:ital,opsz,wght@0,14..32,100..900;1,14..32,100..900&display=swap);html{-webkit-font-smoothing:antialiased;-moz-osx-font-smoothing:grayscale;text-rendering:optimizeLegibility}svg{shape-rendering:geometricPrecision;text-rendering:geometricPrecision}a,h1,h2,h3,h4,h5,h6,img,p{box-sizing:border-box;-webkit-tap-highlight-color:transparent;-webkit-font-smoothing:inherit;-moz-osx-font-smoothing:inherit;backface-visibility:hidden;transform:translateZ(0);user-drag:none}img{display:block;max-width:100%;image-rendering:crisp-edges;image-rendering:pixelated;-webkit-touch-callout:none;-webkit-user-drag:none;user-select:none}*{border-collapse:inherit}*,:after,:before{box-sizing:inherit}svg{display:inline-block;fill:currentColor}a,button{-webkit-tap-highlight-color:transparent}:where(code,kbd,samp,pre){font-size:1em}canvas,picture,svg,video{max-width:100%;display:block}:where(article,aside,details,figcaption,figure,footer,header,main,menu,nav,section){display:block}a{text-decoration:none}a:hover{text-decoration:underline}.toc a{color:#ffffe3}.toc a:visited{color:#ffffe3}.content a{color:#0af}.content a:visited{color:#0af}html{margin:0;padding:0;padding-bottom:4em;background-color:#10100e;color:#ffffe3!important}body{padding:0 24px;margin:0 auto;max-width:912px;font-size:16px;font-family:Inter,sans-serif;font-optical-sizing:auto;font-weight:400;font-style:normal;line-height:1.5}blockquote{background-color:transparent;border-left:2.5px solid #ffffe3;padding:0 .25em 0 1em;margin:0 0 1em 0;opacity:.5;color:inherit}h1{font-size:4em;line-height:1.25;margin-top:1.5em;margin-bottom:.25em}p{line-height:1.5}h1:first-child{margin-top:.5em}h2{border-bottom:2px solid #ffffe3;padding-bottom:.25em;margin-top:1.5em}h1{break-before:page}h2,h3,h4,h5,h6{break-before:auto}details>p>br{display:none!important}details>summary{color:#ffffe3;cursor:pointer}details{margin-top:2em!important;color:rgba(255,255,227,.5)}nav{margin-top:2em;color:inherit}nav>ul>li>a{opacity:1}nav a{color:inherit;opacity:.5}nav>ul>li{margin-top:1em}nav>ul>li>a{font-weight:600}::selection{background-color:#ffffe3;color:#10100e}.katex,.katex .katex-html,.katex .katex-mathml{line-height:1.5;vertical-align:baseline}.katex{display:inline}.katex .katex-html{display:inline}@media print{.katex{transform:scale(1);transform-origin:left baseline}html{background-color:#fff!important;color:#000!important}h1{margin-top:.25em!important}h2{border-color:#000!important}.toc a{color:#000}.toc a:visited{color:#000!important}details{color:rgba(0,0,0,.5)!important}details>summary{color:#000!important}blockquote{border-color:rgba(0,0,0,.5)!important}}</style><style id="page_style">@page{margin:0;padding:.325in .325in;size:10in 13in}</style><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/katex@0.16.25/dist/katex.min.css" integrity="sha384-WcoG4HRXMzYzfCgiyfrySxx90XSl2rxY5mnVY5TwtWE6KLrArNKn0T/mOgNL0Mmi" crossorigin="anonymous"><script defer="defer" src="https://cdn.jsdelivr.net/npm/katex@0.16.25/dist/katex.min.js" integrity="sha384-J+9dG2KMoiR9hqcFao0IBLwxt6zpcyN68IgwzsCSkbreXUjmNVRhPFTssqdSGjwQ" crossorigin="anonymous"></script><script>document.addEventListener("DOMContentLoaded",function(){const t={displayMode:!1,throwOnError:!1,macros:[],fleqn:!1};function e(e){if("SPAN"!==e.tagName)return;const n=e.classList.contains("display"),o=e.firstChild,l=Object.assign({},t);l.displayMode=n;return katex.renderToString(o.textContent,l)}const n=document.querySelectorAll(".math"),o=n.length;let l=0;function s(t){const a=Math.min(l+50,o);for(;l<a;l++){const t=n[l];t.innerHTML=e(t)}l<o&&("function"==typeof requestIdleCallback?requestIdleCallback(s,{timeout:1e3}):setTimeout(s,0))}"function"==typeof requestIdleCallback?requestIdleCallback(s,{timeout:1e3}):setTimeout(s,0)})</script><script src="https://cdn.jsdelivr.net/npm/katex@0.16.25/dist/contrib/copy-tex.min.js" integrity="sha384-HORx6nWi8j5/mYA+y57/9/CZc5z8HnEw4WUZWy5yOn9ToKBv1l58vJaufFAn9Zzi" crossorigin="anonymous"></script><h1>Table of Contents</h1><nav class="toc" role="doc-toc"><ul><li><a href="#probability-theory" id="toc-probability-theory">Probability Theory</a><ul><li><a href="#references" id="toc-references">References</a><li><a href="#notation" id="toc-notation">Notation</a></ul><li><a href="#probability" id="toc-probability">1. Probability</a><ul><li><a href="#def.-probability" id="toc-def.-probability">Def. Probability</a><li><a href="#thm.-basic-probability-properties" id="toc-thm.-basic-probability-properties">Thm. Basic Probability Properties</a><li><a href="#def.-monotone-increasedecrease" id="toc-def.-monotone-increasedecrease">Def. Monotone Increase/Decrease</a><li><a href="#thm.-continuity-of-probabilities" id="toc-thm.-continuity-of-probabilities">Thm. Continuity of Probabilities</a><li><a href="#def.-uniform-probability-distribution" id="toc-def.-uniform-probability-distribution">Def. Uniform Probability Distribution</a><li><a href="#thm.-inclusion-exclusion" id="toc-thm.-inclusion-exclusion">Thm. Inclusion-Exclusion</a><li><a href="#def.-conditional-probability" id="toc-def.-conditional-probability">Def. Conditional Probability</a><li><a href="#thm.-conditional-probability" id="toc-thm.-conditional-probability">Thm. Conditional Probability</a><li><a href="#thm.-bayes-theorem" id="toc-thm.-bayes-theorem">Thm. Bayes’ Theorem</a><li><a href="#thm.-law-of-total-probability-lotp" id="toc-thm.-law-of-total-probability-lotp">Thm. Law of Total Probability (LOTP)</a><li><a href="#thm.-generalized-bayes-theorem" id="toc-thm.-generalized-bayes-theorem">Thm. Generalized Bayes’ Theorem</a><li><a href="#def.-odds" id="toc-def.-odds">Def. Odds</a><li><a href="#remark.-conditional-probabilities-are-probabilities" id="toc-remark.-conditional-probabilities-are-probabilities">Remark. Conditional Probabilities Are Probabilities</a><li><a href="#thm.-bayes-with-extra-condition" id="toc-thm.-bayes-with-extra-condition">Thm. Bayes with Extra Condition</a><li><a href="#thm.-lotp-with-extra-condition" id="toc-thm.-lotp-with-extra-condition">Thm. LOTP with Extra Condition</a><li><a href="#def.-independence" id="toc-def.-independence">Def. Independence</a><li><a href="#thm.-tfae" id="toc-thm.-tfae">Thm. TFAE</a><li><a href="#thm.-indepence-of" id="toc-thm.-indepence-of">Thm. Indepence of</a><li><a href="#def.-3-independence" id="toc-def.-3-independence">Def. <span class="math inline">3</span>-independence</a><li><a href="#def.-n-independence" id="toc-def.-n-independence">Def. <span class="math inline">n</span>-independence</a><li><a href="#def.-conditional-independence" id="toc-def.-conditional-independence">Def. Conditional Independence</a><li><a href="#remarks" id="toc-remarks">Remarks</a></ul><li><a href="#random-variables" id="toc-random-variables">2. Random Variables</a><ul><li><a href="#def.-random-variable-r.v." id="toc-def.-random-variable-r.v.">Def. Random Variable (R.V.)</a><li><a href="#notation.-random-variable" id="toc-notation.-random-variable">Notation. Random Variable</a><li><a href="#def.-cumulative-distribution-function-cdf" id="toc-def.-cumulative-distribution-function-cdf">Def. Cumulative Distribution Function (CDF)</a><li><a href="#def.-discrete-random-variable" id="toc-def.-discrete-random-variable">Def. Discrete Random Variable</a><li><a href="#def.-probability-mass-function" id="toc-def.-probability-mass-function">Def. Probability Mass Function</a><li><a href="#remark.-notation" id="toc-remark.-notation">Remark. Notation</a><li><a href="#thm.-valid-pmfs" id="toc-thm.-valid-pmfs">Thm. Valid PMFs</a></ul><li><a href="#discrete-distributions" id="toc-discrete-distributions">3. Discrete Distributions</a><ul><li><a href="#def.-bernoulli-distribution" id="toc-def.-bernoulli-distribution">Def. Bernoulli Distribution</a><li><a href="#def.-indicator-random-variable" id="toc-def.-indicator-random-variable">Def. Indicator Random Variable</a><li><a href="#def.-binomial-distribution" id="toc-def.-binomial-distribution">Def. Binomial Distribution</a><li><a href="#thm.-binomial-pmf" id="toc-thm.-binomial-pmf">Thm. Binomial PMF</a><li><a href="#thm." id="toc-thm.">Thm. ~</a><li><a href="#thm.-1" id="toc-thm.-1">Thm. ~</a><li><a href="#thm.-hypergeometric-distribution" id="toc-thm.-hypergeometric-distribution">Thm. Hypergeometric Distribution</a><li><a href="#thm.-2" id="toc-thm.-2">Thm. ~</a><li><a href="#def.-discrite-uniform-distribution" id="toc-def.-discrite-uniform-distribution">Def. Discrite Uniform Distribution</a><li><a href="#def.-cumulative-distribution-function" id="toc-def.-cumulative-distribution-function">Def. Cumulative Distribution Function</a><li><a href="#thm.-valid-cdfs" id="toc-thm.-valid-cdfs">Thm. Valid CDFs</a><li><a href="#def.-function-of-a-random-variable" id="toc-def.-function-of-a-random-variable">Def. Function of a Random Variable</a></ul></ul></nav><div class="content"><h1 id="probability-theory">Probability Theory</h1><p>For now this note, unlike my others, has only become a simple summary for the primary reference below. So if you are trying to learn the topic, reading the reference might a better idea.<h2 id="references">References</h2><ul><li><strong>All of Statistics: A Concise Course in Statistical Inference</strong> by Larry Wasserman<li><strong>Introduction to Probability, 2nd Ed.</strong> by Joseph K. Blitzstein and Jessica Hwang</ul><h2 id="notation">Notation</h2><ul><li><span class="math inline">0 \in \N</span> and <span class="math inline">\N^+ = \N \setminus \{0\}</span>.</ul><h1 id="probability">1. Probability</h1><!-- ## Thm. Identities

        $$\binom{n}{k} = \binom{n}{n - k}$$

        $$n\binom{n-1}{k-1} = k\binom{n}{k}$$

        [**Wikipedia:** Vandermonde's identity](https://en.wikipedia.org/wiki/Vandermonde%27s_identity)

        $$\binom{m+n}{k} = \sum_{j=0}^{k} \binom{m}{j} \binom{n}{k - j}$$

        [**Wikipedia:** Generalized Vandermonde's identity](https://en.wikipedia.org/wiki/Vandermonde%27s_identity#Generalized_Vandermonde's_identity)

        $$\binom{m_1 + \cdots + m_p}{k} = \sum_{j_1 + \dots + j_p = k} \binom{m_1}{j_1} \binom{m_2}{j_2} \cdots \binom{m_p}{j_p}$$ --><h2 id="def.-probability">Def. Probability</h2><p>A <strong>probability space</strong> consists of a sample space <span class="math inline">\Omega</span> and a <strong>probability function</strong> (or <strong>probability distribution</strong> or <strong>probability measure</strong>) <span class="math inline">P</span> maps <strong>event</strong> <span class="math inline">A \subseteq \Omega</span> to <span class="math inline">P(A) \in [0, 1]</span>. The function <span class="math inline">P</span> must satisfy the following axioms:<ul><li><span class="math inline">P(\varnothing) = 0</span> and <span class="math inline">P(\Omega)=1</span>.<li>If <span class="math inline">A_1, A_2 ...</span> are disjoint events (mutually exclusive), then <span class="math display">P\left(\bigcup_{j} A_j\right) = \sum_{j} P(A_j)</span></ul><p>Elements <span class="math inline">\Omega</span> are called <strong>sample outcomes</strong>, <strong>realizations</strong>, or <strong>elements</strong>.<blockquote><p>So basically an event is a subset of the sample space and the probabiliy measure satisfies some simple yet very powerful axioms.</blockquote><h2 id="thm.-basic-probability-properties">Thm. Basic Probability Properties</h2><p>For any events <span class="math inline">A</span> and <span class="math inline">B</span>:<ul><li><span class="math inline">P(A^c) = 1 - P(A)</span>.<li><span class="math inline">A \subseteq B \implies P(A) \leq P(B)</span>.<li><span class="math inline">P(A \cup B) = P(A) + P(B) - P(A \cap B)</span></ul><h2 id="def.-monotone-increasedecrease">Def. Monotone Increase/Decrease</h2><p>A sequence of sets <span class="math inline">A_1, A_2, ...</span> is said to be <strong>monotone increasing</strong> if<p><span class="math display">A_1 \subseteq A_2 \subseteq \cdots</span><p>and <strong>monotone decreasing</strong> if<p><span class="math display">A_1 \supseteq A_2 \supseteq \cdots</span><p>In the former case we define the limit<p><span class="math display">\lim_{n \to \infty} A_n = \bigcup_{i = 1}^{\infty} A_i</span><p>and for the latter case we define<p><span class="math display">\lim_{n \to \infty} A_n = \bigcap_{i = 1}^{\infty} A_i</span><p>Either case is denoted with <span class="math inline">A_n \to A</span>.<h2 id="thm.-continuity-of-probabilities">Thm. Continuity of Probabilities</h2><p>Let <span class="math inline">A_n \to A</span>, then <span class="math inline">P(A_n) \to P(A)</span> as <span class="math inline">n \to \infty</span>.<h2 id="def.-uniform-probability-distribution">Def. Uniform Probability Distribution</h2><p>If the sample space <span class="math inline">\Omega</span> is finite and if each outcome is equally likely, then<p><span class="math display">P(A) = \dfrac{|A|}{|\Omega|}</span><p>so that <span class="math inline">P</span> is called the <strong>uniform probability distribution</strong>.<h2 id="thm.-inclusion-exclusion">Thm. Inclusion-Exclusion</h2><p><span class="math display">\begin{array}{llll} P \left(\&gt; \bigcup_{i=1}^{n} A_i \right) =&amp; +&amp; \sum_{i} &amp; P(A_i) \\ &amp;-&amp; \sum_{i \&gt; &lt; \&gt; j} &amp; P(A_i \cap A_j) \\ &amp;+&amp; \sum_{i \&gt; &lt; \&gt; j \&gt; &lt; k} &amp; P(A_i \cap A_j \cap A_k)\\ &amp;\cdots &amp; (-1)^{n+1} &amp; P(A_1 \cap ... \cap A_n) \end{array}</span><h2 id="def.-conditional-probability">Def. Conditional Probability</h2><p>Let <span class="math inline">A</span> and be <span class="math inline">B</span> be events, the we define the <strong>conditional probability</strong> of <span class="math inline">A</span> given <span class="math inline">B</span> as<p><span class="math display">P(A \mid B) := \frac{P(A \cap B)}{P(B)}</span><h2 id="thm.-conditional-probability">Thm. Conditional Probability</h2><p><span class="math display">P(A \cap B) = P(B) P(A \mid B)=P(A) P(B \mid A)</span> <span class="math display">P(A_1, ... \&gt;, A_n) = P(A_1) P(A_2 \mid A_1)P(A_3 \mid A_2,A_1) \cdots P(A_n \mid A_{n-1}, ... \&gt; , A_1)</span> <span class="math display">P(A_1, A_2, A_3) = P(A_1) P(A_2 \mid A_1) P(A_3 \mid A_2,A_1) = P(A_2) P(A_3 \mid A_2) P(A_1 \mid A_2, A_3)</span><h2 id="thm.-bayes-theorem">Thm. Bayes’ Theorem</h2><p>Let <span class="math inline">A</span> and <span class="math inline">B</span> events, then we have<p><span class="math display">P(A \mid B) = \frac{P(B \mid A)P(A)}{P(B)}</span><p>where <span class="math inline">P(A)</span> is called the <strong>prior</strong> and <span class="math inline">P(A \mid B)</span> is called the <strong>posterior</strong> probability of <span class="math inline">A</span>.<h2 id="thm.-law-of-total-probability-lotp">Thm. Law of Total Probability (LOTP)</h2><p>Let <span class="math inline">A_1, ..., A_n</span> partition the sample space <span class="math inline">S</span>, then<p><span class="math display">P(B) = \sum_{i=1}^{n} P(B \cap A_i) = \sum_{i=1}^{n} P(B \mid A_i) P(A_i)</span><h2 id="thm.-generalized-bayes-theorem">Thm. Generalized Bayes’ Theorem</h2><p>Let <span class="math inline">A_1, ..., A_n</span> be a partition of the sample space <span class="math inline">\Omega</span> such that each <span class="math inline">A_i</span> and <span class="math inline">B</span> has a positive probability, then<p><span class="math display">P(A_i \mid B) = \dfrac{P(B \mid A_i) P(A_i)}{\sum_{j=1}^n P(B \mid A_j) P(A_j)}</span><h2 id="def.-odds">Def. Odds</h2><p><span class="math display">\text{odds}(A) := \frac{P(A)}{P(A^c)} = \frac{P(A)}{1 - P(A)} \implies P(A) = \frac{\text{odds}(A)}{1 + \text{odds}(A)}</span></p><!-- ## Thm. Odds Bayes

        $$
        \begin{array}{ccccc}
        \dfrac{P(A \mid B)}{P(A^c \mid B)}  &=& \dfrac{P(B \mid A)}{P(B \mid A^c)} & \cdot & \dfrac{P(A)}{P(A^c)} \\
        \\
        \small \text{Posterior} & & \small \text{Likelihood} & & \small \text{Prior} \\
        \small \text{Odds} & & \small \text{Ratio} & & \small \text{Odds} \\
        \end{array}
        $$ --><!-- ## Ex. Sensitivity and Specificity

        [**Wikipedia:** Sensitivity and Specificity](https://en.wikipedia.org/wiki/Sensitivity_and_specificity)

        Let $D$ denote the event of true positive and let $T$ denote the event of test is positive. $P(T|D)$ is called
        **sensitivity** or **true positive rate**. $P(T^c|D^c)$ is called **specificity** or **true negative rate**. --><h2 id="remark.-conditional-probabilities-are-probabilities">Remark. Conditional Probabilities Are Probabilities</h2><ul><li><span class="math inline">0 \leq P(A \mid E) \leq 1</span>.<li><span class="math inline">P(\varnothing \mid E) = 0</span> and <span class="math inline">P(S \mid E) = 1</span>.<li>If <span class="math inline">A_1, A_2, ...</span> are disjoint events then <span class="math inline">P(\bigcup_j A_j \mid E) = \sum_{j} P(A_j \mid E)</span>.<li><span class="math inline">P(A^c \mid E) = 1 - P(A \mid E)</span>.<li><strong>(Inclusion-Exclusion)</strong> <span class="math inline">P(A \cup B \mid E) = P(A \mid E) + P(B \mid E) - P(A \cap B \mid E)</span>.</ul><p>So, the conditional probability is also a probability. Similarly, we can see probability as a conditional probability.<h2 id="thm.-bayes-with-extra-condition">Thm. Bayes with Extra Condition</h2><p>Provided <span class="math inline">P(A \cap E) &gt; 0</span> and <span class="math inline">P(B \cap E) &gt; 0</span> we have<p><span class="math display">P(A \mid B, E) = \frac{P(B \mid A, E) P(A \mid E)}{P(B \mid E)}</span><h2 id="thm.-lotp-with-extra-condition">Thm. LOTP with Extra Condition</h2><p>Let <span class="math inline">A_1, ..., A_n</span> partition <span class="math inline">S</span> and <span class="math inline">P(A_i \cap E) &gt; 0</span> for all <span class="math inline">i</span>, then<p><span class="math display">P(B \mid E) = \sum_{i=1}^{n} P(B \mid A_i, E)P(A_i \mid E)</span><h2 id="def.-independence">Def. Independence</h2><p>Two events <span class="math inline">A</span> and <span class="math inline">B</span> are called <strong>independent</strong> if (and only if)<p><span class="math display">P(A \cap B) = P(A)P(B)</span><blockquote><p>Note that independence is <em>completely different</em> from disjointness. Disjoint events <span class="math inline">A</span> and <span class="math inline">B</span> can be independent only if <span class="math inline">P(A)=0</span> or <span class="math inline">P(B)=0</span>.<p>Therefore, just recall that “disjoint events with positive probability are not independent”.</blockquote><h2 id="thm.-tfae">Thm. TFAE</h2><p>The following are equivalent if <span class="math inline">P(A)&gt;0</span> and <span class="math inline">P(B)&gt;0</span>,<ul><li><span class="math inline">A</span> and <span class="math inline">B</span> are independent.<li><span class="math inline">P(A \mid B)=P(A)</span>.<li><span class="math inline">P(B \mid A)=P(B)</span>.</ul><p>So, knowing <span class="math inline">A</span> gives us no information about <span class="math inline">B</span>. This may not be the case with disjointness.<h2 id="thm.-indepence-of">Thm. Indepence of</h2><p>If <span class="math inline">A</span> and <span class="math inline">B</span> are independent events then so are<ul><li><span class="math inline">A</span> and <span class="math inline">B^c</span>,<li><span class="math inline">A^c</span> and <span class="math inline">B</span>,<li><span class="math inline">A^c</span> and <span class="math inline">B^c</span>.</ul><h2 id="def.-3-independence">Def. <span class="math inline">3</span>-independence</h2><p>Events <span class="math inline">A</span>, <span class="math inline">B</span> and <span class="math inline">C</span> are <strong>independent</strong> if</p>$$ <span class="math display">\begin{array}{ll} P(A \cap B) &amp;= P(A) P(B) \\ P(A \cap C) &amp;= P(A) P(C) \\ P(B \cap C) &amp;= P(B) P(C) \\ P(A \cap B \cap C) &amp;= P(A) P(B) P(C) \end{array}</span><p>$$<p>Beware that pairwise independence does not imply independence!<h2 id="def.-n-independence">Def. <span class="math inline">n</span>-independence</h2><p>Events <span class="math inline">A_1, ..., A_n</span> are independent if they are:<ul><li>pairwise independent,<li>triplewise independent,<li>quadruplewise independent,<li>…<li><span class="math inline">P(A \cap ... \cap A_n) = P(A_1) ... P(A_n)</span>.</ul><h2 id="def.-conditional-independence">Def. Conditional Independence</h2><p>Event <span class="math inline">A</span> and <span class="math inline">B</span> are called <strong>conditionally independent</strong> for event <span class="math inline">E</span> if<p><span class="math display">P(A \cap B \mid E) = P(A \mid E) \&gt; P(B \mid E)</span><blockquote><p>Independence does not imply conditional independence and vice versa. Also, if <span class="math inline">A</span> and <span class="math inline">B</span> is conditionally independent for <span class="math inline">E</span>, it may not be the case for <span class="math inline">E^c</span>.</blockquote><h2 id="remarks">Remarks</h2><blockquote><p>TODO: Sigma Algebra, Field and Borel (p. 26)</blockquote><h1 id="random-variables">2. Random Variables</h1><h2 id="def.-random-variable-r.v.">Def. Random Variable (R.V.)</h2><p>A <strong>random variable</strong> <span class="math inline">X</span> is a just map (function)<p><span class="math display">X: \Omega \to \R</span><p>which maps the <em>elements</em> (called outcomes) of <span class="math inline">\Omega</span> to the real line <span class="math inline">\R</span>.<blockquote><p>Technically it must be a <em>measurable</em> function.</blockquote><h2 id="notation.-random-variable">Notation. Random Variable</h2><h2 id="def.-cumulative-distribution-function-cdf">Def. Cumulative Distribution Function (CDF)</h2><p>Given a random variable <span class="math inline">X</span>, the <strong>cumulative distribution function <span class="math inline">F_X</span></strong> is defined as<p><span class="math display">\def\arraystretch{1.25} \begin{array}{rcl} F_X: \enspace \R &amp;\to&amp; [0, 1] \subseteq \R \\ x &amp;\mapsto&amp; P(X \leq x) \end{array}</span><h2 id="def.-discrete-random-variable">Def. Discrete Random Variable</h2><blockquote><p>TODO: Revise this</blockquote><p>A random variable <span class="math inline">X</span> is said to be <strong>discrete</strong> if there is a countable (finite or countably infinite) list of values <span class="math inline">a_1, a_2, ...</span> such that <span class="math inline">P(X=a_j \enspace \text{for some} \enspace j)=1</span>.<p>If <span class="math inline">X</span> is discrete r.v., then the countable set of values <span class="math inline">x</span> such that <span class="math inline">P(X=x) &gt; 0</span> is called the <strong>support</strong> of <span class="math inline">X</span>.<h2 id="def.-probability-mass-function">Def. Probability Mass Function</h2><p>The <strong>probability mass function (PMF)</strong> of a discrete r.v. <span class="math inline">X</span> is the function <span class="math inline">p_X</span> given by<p><span class="math display">p_X (x) = P(X=x)</span><p>Note that this is positive if <span class="math inline">x</span> is in the support of <span class="math inline">X</span> and <span class="math inline">0</span> otherwise.<h2 id="remark.-notation">Remark. Notation</h2><p>We use <span class="math inline">X = x</span> to denote the event <span class="math inline">\{s \in S \&gt; | \&gt; X(s) = x \}</span>. We cannot take the probability of a random variable, only of an event.<h2 id="thm.-valid-pmfs">Thm. Valid PMFs</h2><blockquote><p>TODO: Rewrite</blockquote><p>Let <span class="math inline">X</span> be a discrete random variable with countable support <span class="math inline">x_1, x_2, ...</span> (where each <span class="math inline">x_i</span> is distinct for notational simplicity). The PMF <span class="math inline">p_X</span> of <span class="math inline">X</span> must satisfy the following:<ul><li><span class="math inline">p_X(x) &gt; 0</span> if <span class="math inline">x = x_j</span> and <span class="math inline">p_X(x) = 0</span> otherwise.<li><span class="math inline">\sum_{j=1} p_X(x_j) = 1</span></ul><h1 id="discrete-distributions">3. Discrete Distributions</h1><h2 id="def.-bernoulli-distribution">Def. Bernoulli Distribution</h2><p><a href="https://en.wikipedia.org/wiki/Bernoulli_distribution"><strong>Wikipedia:</strong> Bernoulli Distribution</a><p>A random variable <span class="math inline">X</span> is said to have the <strong>Bernoulli Distribution</strong> with <strong>parameter</strong> <span class="math inline">p</span> if <span class="math inline">P(X = 1) = p</span> and <span class="math inline">P(X = 0) = 1 - p</span>, where <span class="math inline">0 &lt; p &lt; 1</span>.<p>We write this as <span class="math inline">X \thicksim \text{Bern}(p)</span>. The symbol <span class="math inline">\thicksim</span> is read as <strong>“is distributed as”</strong>.<p>The parameter <span class="math inline">p</span> is often called the <strong>success probability</strong> of the <span class="math inline">\text{Bern}(p)</span> distribution.<p>Any random variable whose possible values are <span class="math inline">0</span> and <span class="math inline">1</span> has a <span class="math inline">\text{Bern}(p)</span> distribution.<h2 id="def.-indicator-random-variable">Def. Indicator Random Variable</h2><p>The <strong>indicator random variable</strong> or <strong>Bernoulli random variable</strong> of an event <span class="math inline">A</span> is the random variable which equals <span class="math inline">1</span> if <span class="math inline">A</span> occurs and <span class="math inline">0</span> otherwise. We will denote the indicator random variable of <span class="math inline">A</span> by <span class="math inline">I_A</span>.<p>Note that <span class="math inline">I_A \thicksim \text{Bern}(p)</span> with <span class="math inline">p = P(A)</span>.<h2 id="def.-binomial-distribution">Def. Binomial Distribution</h2><blockquote><p>Todo: Rewrite</blockquote><p><a href="https://en.wikipedia.org/wiki/Binomial_distribution"><strong>Wikipedia:</strong> Binomial Distribution</a><p>Suppose <span class="math inline">n</span> <em>independent</em> Bernoulli trials are performed, each with the same success probability <span class="math inline">p</span>. Let the random variable <span class="math inline">X</span> be the number of successes.<p>The distribution of <span class="math inline">X</span> is called the <strong>Binomial distribution</strong> with parameters <span class="math inline">n \in \N^+</span> and <span class="math inline">p \in [0, 1]</span> denoted by <span class="math inline">X \thicksim \text{Bin}(n, p)</span>.<h2 id="thm.-binomial-pmf">Thm. Binomial PMF</h2><p>If <span class="math inline">X \thicksim \text{Bin}(n, p)</span>, then the PMF of <span class="math inline">X</span> is<p><span class="math display">P(X = k) = \dbinom{n}{k} p^k (1-p)^{n-k}</span><p>for <span class="math inline">k \in \N</span>. and <span class="math inline">k \leq n</span>. If <span class="math inline">k &gt; n</span>, then <span class="math inline">P(X=k)=0</span>.<h2 id="thm.">Thm. ~</h2><p>Let <span class="math inline">X \thicksim \text{Bin}(n, p)</span> and <span class="math inline">q = 1 -p</span>. Then <span class="math inline">n - X \thicksim \text{Bin}(n, q)</span>.<h2 id="thm.-1">Thm. ~</h2><p>Let <span class="math inline">X \thicksim \text{Bin}(n, \frac{1}{2})</span> and <span class="math inline">n</span> even. Then the distribution of <span class="math inline">X</span> is symmetric about <span class="math inline">\frac{n}{2}</span> such that<p><span class="math display">P(X = \frac{n}{2} + j) = P(X = \frac{n}{2} - j)</span><p>for all <span class="math inline">j \geq 0</span>.<h2 id="thm.-hypergeometric-distribution">Thm. Hypergeometric Distribution</h2><blockquote><p>Todo: Further define</blockquote><p><a href="https://en.wikipedia.org/wiki/Hypergeometric_distribution"><strong>Wikipedia:</strong> Hypergeometric Distribution</a><p>If <span class="math inline">X \thicksim \text{HGeom}(w, b, n)</span>, then the PMF of <span class="math inline">X</span> is<p><span class="math display">P(X = k) = \dfrac{\dbinom{w}{k} \dbinom{b}{n- k}}{\dbinom{w+b}{n}}</span><p>for <span class="math inline">0 \leq k \leq w</span> and <span class="math inline">0 \leq n-k \leq b</span>, and <span class="math inline">P(X=k)=0</span> otherwise.<h2 id="thm.-2">Thm. ~</h2><p>The distributions <span class="math inline">\text{HGeom}(w, b, n)</span> and <span class="math inline">\text{HGeom}(n, w + b -n, w)</span> are identical.<h2 id="def.-discrite-uniform-distribution">Def. Discrite Uniform Distribution</h2><p><a href="https://en.wikipedia.org/wiki/Discrete_uniform_distribution"><strong>Wikipedia:</strong> Discrete Uniform Distribution</a><p>The PMF of <span class="math inline">X \thicksim \text{DUnif}(C)</span> is<p><span class="math display">P(X=x) = \dfrac{1}{|C|}</span><p>for <span class="math inline">x \in C</span> and <span class="math inline">0</span> otherwise.<h2 id="def.-cumulative-distribution-function">Def. Cumulative Distribution Function</h2><p>The <strong>cumulative distribution function</strong> of an random variable <span class="math inline">X</span> (not necessarily discrete) is the function <span class="math inline">F_X</span> where<p><span class="math display">F_X(x) =P(X \leq x)</span><h2 id="thm.-valid-cdfs">Thm. Valid CDFs</h2><p>For any CDF <span class="math inline">F_X</span>, or simply <span class="math inline">F</span>, we have<ul><li><span class="math inline">x_1 \leq x_2 \implies F(x_1) \leq F(x_2)</span><li><span class="math inline">F(a) = \lim_{x \to a^+} F(x)</span><li><span class="math inline">\lim_{x \to - \infty} F(x) = 0</span><li><span class="math inline">\lim_{x \to \infty} F(x) = 1</span></ul><div class="sourceCode" id="cb1"><pre class="sourceCode js"><code class="sourceCode javascript"></code></pre></div><div class="sourceCode" id="cb2"><pre class="sourceCode js"><code class="sourceCode javascript"></code></pre></div><h2 id="def.-function-of-a-random-variable">Def. Function of a Random Variable</h2><p>For a random variable <span class="math inline">X</span> in the sample space <span class="math inline">S</span> and a function <span class="math inline">h: \R \to \R</span> the random variable <span class="math inline">h(X)</span> maps <span class="math inline">s \in S</span> to <span class="math inline">h(X(s))</span>.</div>